#!/bin/bash
#SBATCH --job-name=WolfTuned
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=3
#SBATCH --ntasks-per-node=1
#SBATCH --time=3:00:00
#SBATCH --mem=60000M
#SBATCH --partition=gpu_shared_course
#SBATCH --gres=gpu:1
module purge
module load eb
module load Python/3.6.3-foss-2017b
module load cuDNN/7.0.5-CUDA-9.0.176
module load NCCL/2.0.5-CUDA-9.0.176
export LD_LIBRARY_PATH=/hpc/eb/Debian9/cuDNN/7.1-CUDA-8.0.44-GCCcore5.4.0/lib64:$LD_LIBRARY_PATH
srun date &> log_tuned.txt
srun python traintuned.py --txt_file alice.txt --learning_rate_decay 0.9 --learning_rate 5e-3 --dropout_keep_prob 0.5 --batch_size 512 --seq_length 90 --lstm_num_layers 4 --lstm_num_hidden 252 >> log_tuned.txt
srun date >> log_tuned.txt
